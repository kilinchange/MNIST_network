# 神经网络整体框架

![神经网络框架](D:\DCj\CV\mnist\pictures\神经网络框架.png)

> -  General Activation Formula: $a_j^{[l]}=g^{[l]}(\sum_kw_{jk}a_k^{[l-1]}+b_j^{[l]})=g^{[l]}(z_j^{[l]})$ where $g^{[l]}$ denotes the $l^{th}$ layer activation function

# 代码实现

- **初始化网络：``__init__(self, sizes)``**

  初始化神经网络的层数（``num_layers``,含输入层）、各层维度（``sizes``）、各层偏置（``biases``）、各层权重（``weights``）。

  ```python
      def __init__(self, sizes):
          self.num_layers = len(sizes)
          self.sizes = sizes
          self.biases = [np.random.randn(x, 1) for x in sizes[1:]]
          self.weights = [np.random.randn(x, y) for x, y in zip(sizes[1:], sizes[:-1])]
  ```

  

- **前向传播：``feedforward(self, a)``**

  > Return the output ==of the network== if "a" is input. 

  ```python
      def feedforward(self, a):
          for b, w in zip(self.biases, self.weights):
              a = sigmoid(w @ a + b)
          return a
  ```

- **随机梯度下降：``SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None``**

  ``SGD``将训练集拆分为大小为``mini_batch_size``的``batch``，利用每一个``batch``更新一次神经网络的偏置和权重参数，每次更新即为一次``iteration``，经过$training\_data\_size/mini\_batch\_size$次``iteration``后，所有训练集的数据都被利用了一次，我们称为一次``epoch``（每个``epoch``都会重新拆分``batch``）。

  在$Bengio$参与撰写的$Deep\ Learning\ Book$中提到：

  > ​	The standard error of the mean is given by
  > $$
  > SE(\hat{\mu}_m)=\sqrt{Var[\frac{1}{m}\sum_{t=1}^mx^{(i)}]}=\frac{\sigma}{\sqrt{m}}
  > $$
  > where $\sigma$ is the true variance of the samples $x^i$.

  > ​	The standard error of the mean is very useful in machine learning experiments. We often estimate the generalization error by computing ==the sample mean of the error== on the test set.